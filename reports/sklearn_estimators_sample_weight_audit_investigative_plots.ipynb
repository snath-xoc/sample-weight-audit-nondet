{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn sample_weight compliance investigative plots\n",
    "\n",
    "This notebook runs compliance tests on a given scikit-learn estimators. This allows one to plot different features of the resulting predictions and scores with and without weights, so as to check weighted-repeated equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import threadpoolctl\n",
    "\n",
    "# HistGradientBoostingClassifier trashes the OpenMP thread pool on repeated\n",
    "# small fits.\n",
    "_ = threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:09<00:09,  5.23it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sample_weight_audit import check_weighted_repeated_estimator_fit_equivalence\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Note: strong regularization (low C value) makes LogisticRegression\n",
    "# predictions sensitive to number of data points: this can be a problem when\n",
    "# used in conjunction with BaggingClassifier, which resamples for a given\n",
    "# number of data points, that can be defined either as a fraction of the\n",
    "# training set or as an absolute number.\n",
    "# solver = \"saga\"  # stochastic, but does not converge for small C values.\n",
    "solver = \"lbfgs\"  # deterministic and stable on this dataset.\n",
    "estimator = LogisticRegression(C=1e-3, solver=solver, max_iter=1_000)\n",
    "# estimator = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# max_samples = 1.0  # relative number of data points to resample during bagging.\n",
    "max_samples = 100  # absolute number of data points to resample during bagging.\n",
    "estimator = BaggingClassifier(estimator, n_estimators=30, max_samples=max_samples)\n",
    "estimator_name = estimator.__class__.__name__\n",
    "\n",
    "result = check_weighted_repeated_estimator_fit_equivalence(\n",
    "    estimator,\n",
    "    estimator_name,\n",
    "    test_name=\"kstest\",\n",
    "    n_stochastic_fits=100,\n",
    "    n_samples_per_cv_group=300,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plots of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "alpha = 0.5\n",
    "eps = 1e-6\n",
    "min_score = min(result.scores_repeated.min(), result.scores_weighted.min())\n",
    "max_score = max(result.scores_repeated.max(), result.scores_weighted.max())\n",
    "if max_score - min_score < eps:\n",
    "    min_score -= eps\n",
    "    max_score += eps\n",
    "\n",
    "bins_scores = np.linspace(min_score, max_score, 15)\n",
    "ax.hist(result.scores_repeated, alpha=alpha, bins=bins_scores, label=\"repeated\")\n",
    "ax.hist(result.scores_weighted, alpha=alpha, bins=bins_scores, label=\"weighted\")\n",
    "ax.legend()\n",
    "_ = ax.set_title(f\"KS p-value: {result.pvalue:.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plots of predictions\n",
    "\n",
    "## Plot the histograms of predictions for some randomly selected test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 4, figsize=(14, 12))\n",
    "n_plots = len(axs.flatten())\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "sample_indices = rng.choice(\n",
    "    np.arange(result.predictions_weighted.shape[1]), size=n_plots, replace=False\n",
    ")\n",
    "\n",
    "min_prediction = min(\n",
    "    result.predictions_repeated.min(), result.predictions_weighted.min()\n",
    ")\n",
    "max_prediction = max(\n",
    "    result.predictions_repeated.max(), result.predictions_weighted.max()\n",
    ")\n",
    "if max_prediction - min_prediction < eps:\n",
    "    min_prediction -= eps\n",
    "    max_prediction += eps\n",
    "bins_predictions = np.linspace(min_prediction, max_prediction, 30)\n",
    "\n",
    "for sample_idx, ax in zip(sample_indices, axs.flatten()):\n",
    "    ax.hist(\n",
    "        result.predictions_repeated[:, sample_idx].flatten(),\n",
    "        alpha=alpha,\n",
    "        bins=bins_predictions,\n",
    "        label=\"repeated\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        result.predictions_weighted[:, sample_idx].flatten(),\n",
    "        alpha=alpha,\n",
    "        bins=bins_predictions,\n",
    "        label=\"weighted\",\n",
    "    )\n",
    "\n",
    "# Show the legend only on the last subplot\n",
    "_ = ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zooming into individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8, 6))\n",
    "\n",
    "sample_idx = 50\n",
    "ax.hist(result.predictions_repeated[:, sample_idx].flatten(), alpha=alpha, bins=bins_predictions, label='repeated')\n",
    "ax.hist(result.predictions_weighted[:, sample_idx].flatten(), alpha=alpha, bins=bins_predictions, label='weighted')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
