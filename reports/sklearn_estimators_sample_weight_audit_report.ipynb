{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn sample_weight compliance report\n",
    "\n",
    "This notebook runs compliance tests on all scikit-learn estimators. Estimator as inspected to check whether they are expected to have a stochastic fit or not. If the fit is stochastic, a dedicated statistical test is performed, otherwise a deterministic estimator check is run instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]\n",
      "executable: /Users/ogrisel/miniforge3/envs/dev/bin/python\n",
      "   machine: macOS-15.2-arm64-arm-64bit\n",
      "\n",
      "Python dependencies:\n",
      "      sklearn: 1.7.dev0\n",
      "          pip: 25.0\n",
      "   setuptools: 75.8.0\n",
      "        numpy: 2.1.3\n",
      "        scipy: 1.15.1\n",
      "       Cython: 3.0.11\n",
      "       pandas: 2.2.3\n",
      "   matplotlib: 3.10.0\n",
      "       joblib: 1.5.dev0\n",
      "threadpoolctl: 3.5.0\n",
      "\n",
      "Built with OpenMP: True\n",
      "\n",
      "threadpoolctl info:\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "    num_threads: 8\n",
      "         prefix: libomp\n",
      "       filepath: /Users/ogrisel/miniforge3/envs/dev/lib/libomp.dylib\n",
      "        version: None\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "import traceback\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.utils.estimator_checks import check_sample_weight_equivalence_on_dense_data\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import threadpoolctl\n",
    "\n",
    "from sample_weight_audit import check_weighted_repeated_estimator_fit_equivalence\n",
    "from sample_weight_audit.exceptions import UnexpectedDeterministicPredictions\n",
    "from sample_weight_audit.sklearn_stochastic_params import STOCHASTIC_FIT_PARAMS\n",
    "\n",
    "# HistGradientBoostingClassifier trashes the OpenMP thread pool on repeated\n",
    "# small fits.\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # division by zero in AdaBoost\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # liblinear can fail to converge\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # KBinsDiscretizer with collapsed bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "ESTIMATORS_TO_SKIP = [\n",
    "    LogisticRegressionCV,  # too slow and already somewhat tested by LogisticRegression\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ ARDRegression does not support sample_weight\n",
      "Evaluating AdaBoostClassifier(estimator=DecisionTreeClassifier(max_features=0.5,\n",
      "                                                    min_weight_fraction_leaf=0.1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AdaBoostClassifier: (p_value: 0.815)\n",
      "Evaluating AdaBoostRegressor(estimator=DecisionTreeRegressor(max_features=0.5,\n",
      "                                                  min_weight_fraction_leaf=0.1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:02<00:03, 17.01it/s]"
     ]
    }
   ],
   "source": [
    "N_STOCHASTIC_FITS = 100\n",
    "TEST_THRESHOLD = 0.05\n",
    "\n",
    "\n",
    "statistical_test_results = []\n",
    "deterministic_test_results = []\n",
    "missing_sample_weight_support = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "for est_name, est_class in all_estimators(\n",
    "    type_filter=[\"classifier\", \"regressor\", \"cluster\", \"transformer\"]\n",
    "):\n",
    "    if est_class in ESTIMATORS_TO_SKIP:\n",
    "        print(f\"Skipping {est_name}\")\n",
    "        continue\n",
    "\n",
    "    if \"sample_weight\" not in signature(est_class.fit).parameters:\n",
    "        print(f\"⚠ {est_name} does not support sample_weight\")\n",
    "        missing_sample_weight_support.append(est_name)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        est = est_class(**STOCHASTIC_FIT_PARAMS.get(est_class, {}))\n",
    "    except TypeError as e:\n",
    "        print(f\"⚠ {est_name} failed to instantiate: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"random_state\" not in est.get_params():\n",
    "        # TODO: leverage sklearn's PER_ESTIMATOR_CHECKS_PARAMS config to run\n",
    "        # this check on valid parametrizations for the deterministic case.\n",
    "        try:\n",
    "            check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
    "            print(f\"✅ {est} passed the deterministic check\")\n",
    "            deterministic_test_results.append((est, None))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {est} failed the deterministic check\")\n",
    "            deterministic_test_results.append((est, e))\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(f\"Evaluating {est}\")\n",
    "    try:\n",
    "        result = check_weighted_repeated_estimator_fit_equivalence(\n",
    "            est,\n",
    "            test_name=\"kstest\",\n",
    "            n_stochastic_fits=N_STOCHASTIC_FITS,\n",
    "            random_state=0,\n",
    "        )\n",
    "        pass_or_fail = \"✅\" if result.p_value > TEST_THRESHOLD else \"❌\"\n",
    "        print(\n",
    "            f\"{pass_or_fail} {est_name}: (p_value: {result.p_value:.3f})\"\n",
    "        )\n",
    "        statistical_test_results.append(result)\n",
    "    except UnexpectedDeterministicPredictions:\n",
    "        # The estimator parametrization led to deterministic behavior, which is\n",
    "        # unexpected. Run the deterministic check to investigate instead.\n",
    "        print(f\"⚠ {est_name} with different random states led to the same predictions\")\n",
    "        try:\n",
    "            check_sample_weight_equivalence_on_dense_data(\n",
    "                est_name, est.set_params(random_state=0)\n",
    "            )\n",
    "            print(f\"✅ {est} passed the deterministic check\")\n",
    "            deterministic_test_results.append((est, None))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {est} failed the deterministic check\")\n",
    "            deterministic_test_results.append((est, e))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {est} error with: {e}\")\n",
    "        errors.append((est, e))\n",
    "\n",
    "results_df = pd.DataFrame([r.to_dict() for r in statistical_test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"✅ {len([r for r in deterministic_test_results if r[1] is None])} \"\n",
    "    \"passed the deterministic test\"\n",
    ")\n",
    "print(\n",
    "    f\"❌ {len([r for r in deterministic_test_results if r[1] is not None])} \"\n",
    "    \"failed the deterministic test\"\n",
    ")\n",
    "print(\n",
    "    f\"✅ {len([r for r in statistical_test_results if r.p_value > TEST_THRESHOLD])} \"\n",
    "    \"passed the statistical test\"\n",
    ")\n",
    "print(\n",
    "    f\"❌ {len([r for r in statistical_test_results if r.p_value <= TEST_THRESHOLD])} \"\n",
    "    \"failed the statistical test\"\n",
    ")\n",
    "print(f\"❌ {len(errors)} other errors\")\n",
    "print(\n",
    "    f\"⚠ {len(missing_sample_weight_support)} estimators lack sample_weight \"\n",
    "    \"support\"\n",
    ")\n",
    "results_df = pd.DataFrame([r.to_dict() for r in statistical_test_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on the statistical test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(\"p_value\")[[\"estimator_name\", \"p_value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on deterministic test errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for est, e in deterministic_test_results:\n",
    "    if e is None:\n",
    "        continue\n",
    "\n",
    "    print(f\"❌ {est}: {e}\")\n",
    "    traceback.print_exception(e, file=sys.stdout)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on other errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for est, e in errors:\n",
    "    print(f\"❌ {est}: {e}\")\n",
    "    traceback.print_exception(e, file=sys.stdout)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of estimators with missing sample_weight support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est_name in missing_sample_weight_support:\n",
    "    print(est_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
