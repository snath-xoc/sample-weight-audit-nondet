{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn sample_weight compliance report\n",
    "\n",
    "This notebook runs compliance tests on all scikit-learn estimators. Estimator as inspected to check whether they are expected to have a stochastic fit or not. If the fit is stochastic, a dedicated statistical test is performed, otherwise a deterministic estimator check is run instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ]\n",
      "executable: /Users/shrutinath/micromamba/envs/scikit-learn/bin/python\n",
      "   machine: macOS-14.3-arm64-arm-64bit\n",
      "\n",
      "Python dependencies:\n",
      "      sklearn: 1.7.dev0\n",
      "          pip: 24.0\n",
      "   setuptools: 75.8.0\n",
      "        numpy: 2.0.0\n",
      "        scipy: 1.14.0\n",
      "       Cython: 3.0.10\n",
      "       pandas: 2.2.2\n",
      "   matplotlib: 3.9.0\n",
      "       joblib: 1.4.2\n",
      "threadpoolctl: 3.5.0\n",
      "\n",
      "Built with OpenMP: True\n",
      "\n",
      "threadpoolctl info:\n",
      "       user_api: blas\n",
      "   internal_api: openblas\n",
      "    num_threads: 8\n",
      "         prefix: libopenblas\n",
      "       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libopenblas.0.dylib\n",
      "        version: 0.3.27\n",
      "threading_layer: openmp\n",
      "   architecture: VORTEX\n",
      "\n",
      "       user_api: openmp\n",
      "   internal_api: openmp\n",
      "    num_threads: 8\n",
      "         prefix: libomp\n",
      "       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libomp.dylib\n",
      "        version: None\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "import traceback\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.base import is_clusterer\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.utils.estimator_checks import check_sample_weight_equivalence_on_dense_data\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import threadpoolctl\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,\"../src/\")\n",
    "from sample_weight_audit import check_weighted_repeated_estimator_fit_equivalence\n",
    "from sample_weight_audit.exceptions import UnexpectedDeterministicPredictions\n",
    "from sample_weight_audit.sklearn_stochastic_params import STOCHASTIC_FIT_PARAMS\n",
    "\n",
    "# HistGradientBoostingClassifier trashes the OpenMP thread pool on repeated\n",
    "# small fits.\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # division by zero in AdaBoost\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)  # liblinear can fail to converge\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # KBinsDiscretizer with collapsed bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "ESTIMATORS_TO_SKIP = [\n",
    "    LogisticRegressionCV,  # too slow and already somewhat tested by LogisticRegression\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ ARDRegression does not support sample_weight\n",
      "Evaluating AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1,\n",
      "                                                    max_features=0.5))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AdaBoostClassifier: (p_value: 1.000\n",
      "Evaluating AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=1,\n",
      "                                                  max_features=0.5))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AdaBoostRegressor: (p_value: 0.808\n",
      "⚠ AdditiveChi2Sampler does not support sample_weight\n",
      "⚠ AffinityPropagation does not support sample_weight\n",
      "⚠ AgglomerativeClustering does not support sample_weight\n",
      "Evaluating BaggingClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ BaggingClassifier: (p_value: 0.000\n",
      "Evaluating BaggingRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 64.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ BaggingRegressor: (p_value: 0.000\n",
      "✅ BayesianRidge() passed the deterministic check\n",
      "✅ BernoulliNB() passed the deterministic check\n",
      "⚠ BernoulliRBM does not support sample_weight\n",
      "⚠ Binarizer does not support sample_weight\n",
      "⚠ Birch does not support sample_weight\n",
      "Evaluating BisectingKMeans()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 290.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BisectingKMeans: (p_value: 0.808\n",
      "⚠ CCA does not support sample_weight\n",
      "✅ CalibratedClassifierCV() passed the deterministic check\n",
      "✅ CategoricalNB() passed the deterministic check\n",
      "⚠ ClassifierChain does not support sample_weight\n",
      "⚠ ColumnTransformer does not support sample_weight\n",
      "✅ ComplementNB() passed the deterministic check\n",
      "✅ DBSCAN() passed the deterministic check\n",
      "Evaluating DecisionTreeClassifier(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 591.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DecisionTreeClassifier: (p_value: 1.000\n",
      "Evaluating DecisionTreeRegressor(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 864.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DecisionTreeRegressor: (p_value: 0.594\n",
      "⚠ DictVectorizer does not support sample_weight\n",
      "⚠ DictionaryLearning does not support sample_weight\n",
      "Evaluating DummyClassifier(strategy='stratified')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 817.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DummyClassifier: (p_value: 1.000\n",
      "✅ DummyRegressor() passed the deterministic check\n",
      "Evaluating ElasticNet(selection='random')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 910.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ElasticNet: (p_value: 1.000\n",
      "Evaluating ElasticNetCV(selection='random')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 49.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ElasticNetCV: (p_value: 1.000\n",
      "Evaluating ExtraTreeClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 568.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ExtraTreeClassifier: (p_value: 1.000\n",
      "Evaluating ExtraTreeRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 818.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ExtraTreeRegressor: (p_value: 0.594\n",
      "Evaluating ExtraTreesClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ExtraTreesClassifier: (p_value: 1.000\n",
      "Evaluating ExtraTreesRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ExtraTreesRegressor: (p_value: 0.958\n",
      "⚠ FactorAnalysis does not support sample_weight\n",
      "⚠ FastICA does not support sample_weight\n",
      "⚠ FeatureAgglomeration does not support sample_weight\n",
      "⚠ FeatureHasher does not support sample_weight\n",
      "⚠ FeatureUnion does not support sample_weight\n",
      "⚠ FixedThresholdClassifier does not support sample_weight\n",
      "⚠ FunctionTransformer does not support sample_weight\n",
      "✅ GammaRegressor() passed the deterministic check\n",
      "✅ GaussianNB() passed the deterministic check\n",
      "⚠ GaussianProcessClassifier does not support sample_weight\n",
      "⚠ GaussianProcessRegressor does not support sample_weight\n",
      "⚠ GaussianRandomProjection does not support sample_weight\n",
      "⚠ GenericUnivariateSelect does not support sample_weight\n",
      "Evaluating GradientBoostingClassifier(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:05<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GradientBoostingClassifier: (p_value: 0.594\n",
      "Evaluating GradientBoostingRegressor(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 28.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GradientBoostingRegressor: (p_value: 0.808\n",
      "⚠ HDBSCAN does not support sample_weight\n",
      "⚠ HashingVectorizer does not support sample_weight\n",
      "Evaluating HistGradientBoostingClassifier(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ HistGradientBoostingClassifier: (p_value: 0.000\n",
      "Evaluating HistGradientBoostingRegressor(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 47.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ HistGradientBoostingRegressor: (p_value: 0.000\n",
      "❌ HuberRegressor() failed the deterministic check\n",
      "⚠ IncrementalPCA does not support sample_weight\n",
      "⚠ Isomap does not support sample_weight\n",
      "❌ IsotonicRegression() failed the deterministic check\n",
      "Evaluating KBinsDiscretizer(encode='ordinal', subsample=50)\n",
      "❌ KBinsDiscretizer(encode='ordinal', subsample=50) error with: sample_weight.shape == (100,), expected (50,)!\n",
      "Evaluating KMeans()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 329.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KMeans: (p_value: 1.000\n",
      "⚠ KNNImputer does not support sample_weight\n",
      "⚠ KNeighborsClassifier does not support sample_weight\n",
      "⚠ KNeighborsRegressor does not support sample_weight\n",
      "⚠ KNeighborsTransformer does not support sample_weight\n",
      "⚠ KernelCenterer does not support sample_weight\n",
      "⚠ KernelPCA does not support sample_weight\n",
      "✅ KernelRidge() passed the deterministic check\n",
      "⚠ LabelBinarizer does not support sample_weight\n",
      "⚠ LabelEncoder does not support sample_weight\n",
      "⚠ LabelPropagation does not support sample_weight\n",
      "⚠ LabelSpreading does not support sample_weight\n",
      "⚠ Lars does not support sample_weight\n",
      "⚠ LarsCV does not support sample_weight\n",
      "Evaluating Lasso(selection='random')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 867.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lasso: (p_value: 1.000\n",
      "Evaluating LassoCV(selection='random')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 53.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LassoCV: (p_value: 1.000\n",
      "⚠ LassoLars does not support sample_weight\n",
      "⚠ LassoLarsCV does not support sample_weight\n",
      "⚠ LassoLarsIC does not support sample_weight\n",
      "⚠ LatentDirichletAllocation does not support sample_weight\n",
      "⚠ LinearDiscriminantAnalysis does not support sample_weight\n",
      "✅ LinearRegression() passed the deterministic check\n",
      "Evaluating LinearSVC(dual=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ LinearSVC(dual=True) error with: continuous-multioutput format is not supported\n",
      "Evaluating LinearSVR(dual=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 1080.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LinearSVR: (p_value: 0.594\n",
      "⚠ LocallyLinearEmbedding does not support sample_weight\n",
      "Evaluating LogisticRegression(dual=True, max_iter=10000, solver='liblinear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 125.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ LogisticRegression: (p_value: 0.000\n",
      "Skipping LogisticRegressionCV\n",
      "Evaluating MLPClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLPClassifier: (p_value: 1.000\n",
      "Evaluating MLPRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLPRegressor: (p_value: 1.000\n",
      "⚠ MaxAbsScaler does not support sample_weight\n",
      "⚠ MeanShift does not support sample_weight\n",
      "⚠ MinMaxScaler does not support sample_weight\n",
      "⚠ MiniBatchDictionaryLearning does not support sample_weight\n",
      "Evaluating MiniBatchKMeans(reassignment_ratio=0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 194.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MiniBatchKMeans: (p_value: 0.594\n",
      "⚠ MiniBatchNMF does not support sample_weight\n",
      "⚠ MiniBatchSparsePCA does not support sample_weight\n",
      "⚠ MissingIndicator does not support sample_weight\n",
      "⚠ MultiLabelBinarizer does not support sample_weight\n",
      "⚠ MultiOutputClassifier failed to instantiate: MultiOutputClassifier.__init__() missing 1 required positional argument: 'estimator'\n",
      "⚠ MultiOutputRegressor failed to instantiate: MultiOutputRegressor.__init__() missing 1 required positional argument: 'estimator'\n",
      "⚠ MultiTaskElasticNet does not support sample_weight\n",
      "⚠ MultiTaskElasticNetCV does not support sample_weight\n",
      "⚠ MultiTaskLasso does not support sample_weight\n",
      "⚠ MultiTaskLassoCV does not support sample_weight\n",
      "✅ MultinomialNB() passed the deterministic check\n",
      "⚠ NMF does not support sample_weight\n",
      "⚠ NearestCentroid does not support sample_weight\n",
      "⚠ NeighborhoodComponentsAnalysis does not support sample_weight\n",
      "⚠ Normalizer does not support sample_weight\n",
      "Evaluating NuSVC(probability=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 123.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ NuSVC: (p_value: 0.000\n",
      "❌ NuSVR() failed the deterministic check\n",
      "⚠ Nystroem does not support sample_weight\n",
      "⚠ OPTICS does not support sample_weight\n",
      "⚠ OneHotEncoder does not support sample_weight\n",
      "⚠ OneVsOneClassifier does not support sample_weight\n",
      "⚠ OneVsRestClassifier does not support sample_weight\n",
      "⚠ OrdinalEncoder does not support sample_weight\n",
      "⚠ OrthogonalMatchingPursuit does not support sample_weight\n",
      "⚠ OrthogonalMatchingPursuitCV does not support sample_weight\n",
      "⚠ OutputCodeClassifier does not support sample_weight\n",
      "⚠ PCA does not support sample_weight\n",
      "⚠ PLSCanonical does not support sample_weight\n",
      "⚠ PLSRegression does not support sample_weight\n",
      "⚠ PLSSVD does not support sample_weight\n",
      "⚠ PassiveAggressiveClassifier does not support sample_weight\n",
      "⚠ PassiveAggressiveRegressor does not support sample_weight\n",
      "⚠ PatchExtractor does not support sample_weight\n",
      "Evaluating Perceptron()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Perceptron() error with: continuous-multioutput format is not supported\n",
      "✅ PoissonRegressor() passed the deterministic check\n",
      "⚠ PolynomialCountSketch does not support sample_weight\n",
      "⚠ PolynomialFeatures does not support sample_weight\n",
      "⚠ PowerTransformer does not support sample_weight\n",
      "⚠ QuadraticDiscriminantAnalysis does not support sample_weight\n",
      "✅ QuantileRegressor() passed the deterministic check\n",
      "⚠ QuantileTransformer does not support sample_weight\n",
      "Evaluating RANSACRegressor()\n",
      "❌ RANSACRegressor() error with: Weights sum to zero, can't be normalized\n",
      "⚠ RBFSampler does not support sample_weight\n",
      "⚠ RFE does not support sample_weight\n",
      "⚠ RFECV does not support sample_weight\n",
      "⚠ RadiusNeighborsClassifier does not support sample_weight\n",
      "⚠ RadiusNeighborsRegressor does not support sample_weight\n",
      "⚠ RadiusNeighborsTransformer does not support sample_weight\n",
      "Evaluating RandomForestClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ RandomForestClassifier: (p_value: 0.000\n",
      "Evaluating RandomForestRegressor(max_features=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ RandomForestRegressor: (p_value: 0.000\n",
      "Evaluating RandomTreesEmbedding(n_estimators=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 82.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomTreesEmbedding: (p_value: 0.808\n",
      "⚠ RegressorChain does not support sample_weight\n",
      "Evaluating Ridge(solver='sag')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 182.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Ridge: (p_value: 0.000\n",
      "✅ RidgeCV() passed the deterministic check\n",
      "Evaluating RidgeClassifier(solver='saga')\n",
      "❌ RidgeClassifier(solver='saga') error with: Floating-point under-/overflow occurred at epoch #903. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "✅ RidgeClassifierCV() passed the deterministic check\n",
      "⚠ RobustScaler does not support sample_weight\n",
      "Evaluating SGDClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ SGDClassifier() error with: continuous-multioutput format is not supported\n",
      "Evaluating SGDRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 707.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ SGDRegressor: (p_value: 0.000\n",
      "Evaluating SVC(probability=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 148.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ SVC: (p_value: 0.000\n",
      "❌ SVR() failed the deterministic check\n",
      "⚠ SelectFdr does not support sample_weight\n",
      "⚠ SelectFpr does not support sample_weight\n",
      "⚠ SelectFromModel does not support sample_weight\n",
      "⚠ SelectFwe does not support sample_weight\n",
      "⚠ SelectKBest does not support sample_weight\n",
      "⚠ SelectPercentile does not support sample_weight\n",
      "⚠ SelfTrainingClassifier does not support sample_weight\n",
      "⚠ SequentialFeatureSelector does not support sample_weight\n",
      "⚠ SimpleImputer does not support sample_weight\n",
      "⚠ SkewedChi2Sampler does not support sample_weight\n",
      "⚠ SparseCoder does not support sample_weight\n",
      "⚠ SparsePCA does not support sample_weight\n",
      "⚠ SparseRandomProjection does not support sample_weight\n",
      "⚠ SpectralClustering does not support sample_weight\n",
      "✅ SplineTransformer() passed the deterministic check\n",
      "⚠ StackingClassifier failed to instantiate: StackingClassifier.__init__() missing 1 required positional argument: 'estimators'\n",
      "⚠ StackingRegressor failed to instantiate: StackingRegressor.__init__() missing 1 required positional argument: 'estimators'\n",
      "✅ StandardScaler() passed the deterministic check\n",
      "⚠ TSNE does not support sample_weight\n",
      "⚠ TargetEncoder does not support sample_weight\n",
      "⚠ TfidfTransformer does not support sample_weight\n",
      "⚠ TheilSenRegressor does not support sample_weight\n",
      "⚠ TransformedTargetRegressor does not support sample_weight\n",
      "⚠ TruncatedSVD does not support sample_weight\n",
      "⚠ TunedThresholdClassifierCV does not support sample_weight\n",
      "✅ TweedieRegressor() passed the deterministic check\n",
      "⚠ VarianceThreshold does not support sample_weight\n",
      "⚠ VotingClassifier failed to instantiate: VotingClassifier.__init__() missing 1 required positional argument: 'estimators'\n",
      "⚠ VotingRegressor failed to instantiate: VotingRegressor.__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "statistical_test_results = []\n",
    "deterministic_test_results = []\n",
    "missing_sample_weight_support = []\n",
    "errors = []\n",
    "\n",
    "\n",
    "for est_name, est_class in all_estimators(\n",
    "    type_filter=[\"classifier\", \"regressor\", \"cluster\", \"transformer\"]\n",
    "):\n",
    "    if est_class in ESTIMATORS_TO_SKIP:\n",
    "        print(f\"Skipping {est_name}\")\n",
    "        continue\n",
    "\n",
    "    if \"sample_weight\" not in signature(est_class.fit).parameters:\n",
    "        print(f\"⚠ {est_name} does not support sample_weight\")\n",
    "        missing_sample_weight_support.append(est_name)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        est = est_class(**STOCHASTIC_FIT_PARAMS.get(est_class, {}))\n",
    "    except TypeError as e:\n",
    "        print(f\"⚠ {est_name} failed to instantiate: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"random_state\" not in est.get_params():\n",
    "        # TODO: leverage sklearn's PER_ESTIMATOR_CHECKS_PARAMS config to run\n",
    "        # this check on valid parametrizations for the deterministic case.\n",
    "        try:\n",
    "            check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
    "            print(f\"✅ {est} passed the deterministic check\")\n",
    "            deterministic_test_results.append((est, None))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {est} failed the deterministic check\")\n",
    "            deterministic_test_results.append((est, e))\n",
    "        continue\n",
    "\n",
    "    # XXX: how to handle stochastic clustering estimators?\n",
    "    if is_clusterer(est) and not hasattr(est, \"transform\"):\n",
    "        print(f\"⚠ {est_name} skipped: missing proper handling of clustering estimators\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating {est}\")\n",
    "    try:\n",
    "        result = check_weighted_repeated_estimator_fit_equivalence(\n",
    "            est,\n",
    "            test_name=\"kstest\",\n",
    "            n_stochastic_fits=30,\n",
    "        )\n",
    "        pass_or_fail = \"✅\" if result.p_value > 0.05 else \"❌\"\n",
    "        print(\n",
    "            f\"{pass_or_fail} {est_name}: (p_value: {result.p_value:.3f}\"\n",
    "        )\n",
    "        statistical_test_results.append(result)\n",
    "    except UnexpectedDeterministicPredictions:\n",
    "        # The estimator parametrization led to deterministic behavior, which is\n",
    "        # unexpected. Run the deterministic check to investigate instead.\n",
    "        print(f\"⚠ {est_name} with different random states led to the same predictions\")\n",
    "        try:\n",
    "            check_sample_weight_equivalence_on_dense_data(\n",
    "                est_name, est.set_params(random_state=0)\n",
    "            )\n",
    "            print(f\"✅ {est} passed the deterministic check\")\n",
    "            deterministic_test_results.append((est, None))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {est} failed the deterministic check\")\n",
    "            deterministic_test_results.append((est, e))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {est} error with: {e}\")\n",
    "        errors.append((est, e))\n",
    "\n",
    "results_df = pd.DataFrame([r.to_dict() for r in statistical_test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 19 passed the deterministic test\n",
      "❌ 4 failed the deterministic test\n",
      "✅ 22 passed the statistical test\n",
      "❌ 11 failed the statistical test\n",
      "❌ 6 other errors\n",
      "⚠ 112 estimators lack sample_weight support\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"✅ {len([r for r in deterministic_test_results if r[1] is None])} \"\n",
    "    \"passed the deterministic test\"\n",
    ")\n",
    "print(\n",
    "    f\"❌ {len([r for r in deterministic_test_results if r[1] is not None])} \"\n",
    "    \"failed the deterministic test\"\n",
    ")\n",
    "print(\n",
    "    f\"✅ {len([r for r in statistical_test_results if r.p_value > 0.05])} \"\n",
    "    \"passed the statistical test\"\n",
    ")\n",
    "print(\n",
    "    f\"❌ {len([r for r in statistical_test_results if r.p_value <= 0.05])} \"\n",
    "    \"failed the statistical test\"\n",
    ")\n",
    "print(f\"❌ {len(errors)} other errors\")\n",
    "print(\n",
    "    f\"⚠ {len(missing_sample_weight_support)} estimators lack sample_weight \"\n",
    "    \"support\"\n",
    ")\n",
    "results_df = pd.DataFrame([r.to_dict() for r in statistical_test_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on the statistical test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator_name</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HistGradientBoostingClassifier</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NuSVC</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SVC</td>\n",
       "      <td>1.691123e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>1.014674e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1.014674e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>5.787024e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>2.500012e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ExtraTreeRegressor</td>\n",
       "      <td>5.940706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>5.940706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>5.940706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MiniBatchKMeans</td>\n",
       "      <td>5.940706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LinearSVR</td>\n",
       "      <td>5.940706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>8.079632e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>8.079632e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RandomTreesEmbedding</td>\n",
       "      <td>8.079632e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BisectingKMeans</td>\n",
       "      <td>8.079632e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>9.578463e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLPRegressor</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ElasticNetCV</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    estimator_name       p_value\n",
       "16  HistGradientBoostingClassifier  1.691123e-17\n",
       "30                           Ridge  1.691123e-17\n",
       "28           RandomForestRegressor  1.691123e-17\n",
       "27          RandomForestClassifier  1.691123e-17\n",
       "26                           NuSVC  1.691123e-17\n",
       "31                    SGDRegressor  1.691123e-17\n",
       "32                             SVC  1.691123e-17\n",
       "3                 BaggingRegressor  1.014674e-15\n",
       "22              LogisticRegression  1.014674e-15\n",
       "2                BaggingClassifier  5.787024e-13\n",
       "17   HistGradientBoostingRegressor  2.500012e-07\n",
       "11              ExtraTreeRegressor  5.940706e-01\n",
       "14      GradientBoostingClassifier  5.940706e-01\n",
       "6            DecisionTreeRegressor  5.940706e-01\n",
       "25                 MiniBatchKMeans  5.940706e-01\n",
       "21                       LinearSVR  5.940706e-01\n",
       "15       GradientBoostingRegressor  8.079632e-01\n",
       "1                AdaBoostRegressor  8.079632e-01\n",
       "29            RandomTreesEmbedding  8.079632e-01\n",
       "4                  BisectingKMeans  8.079632e-01\n",
       "13             ExtraTreesRegressor  9.578463e-01\n",
       "18                          KMeans  1.000000e+00\n",
       "7                  DummyClassifier  1.000000e+00\n",
       "24                    MLPRegressor  1.000000e+00\n",
       "19                           Lasso  1.000000e+00\n",
       "20                         LassoCV  1.000000e+00\n",
       "5           DecisionTreeClassifier  1.000000e+00\n",
       "12            ExtraTreesClassifier  1.000000e+00\n",
       "10             ExtraTreeClassifier  1.000000e+00\n",
       "9                     ElasticNetCV  1.000000e+00\n",
       "8                       ElasticNet  1.000000e+00\n",
       "23                   MLPClassifier  1.000000e+00\n",
       "0               AdaBoostClassifier  1.000000e+00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(\"p_value\")[[\"estimator_name\", \"p_value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on deterministic test errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ HuberRegressor(): \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of HuberRegressor.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00051052\n",
      "Max relative difference among violations: 7.5912896\n",
      " ACTUAL: array([-2.323244e-05,  9.999910e-01,  2.000039e+00,  1.202210e+00,\n",
      "        2.430282e+00,  9.999892e-01,  1.999998e+00,  2.000024e+00,\n",
      "        1.656899e+00,  1.999991e+00,  1.576810e+00,  1.295011e+00,\n",
      "        1.829514e+00,  1.000022e+00,  1.000070e+00])\n",
      " DESIRED: array([-2.704185e-06,  9.999971e-01,  2.000005e+00,  1.202141e+00,\n",
      "        2.430112e+00,  9.999871e-01,  1.999991e+00,  2.000007e+00,\n",
      "        1.656642e+00,  1.999968e+00,  1.576735e+00,  1.294886e+00,\n",
      "        1.829381e+00,  1.000010e+00,  9.995597e-01])\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 29, in <module>\n",
      "    check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1576, in check_sample_weight_equivalence_on_dense_data\n",
      "    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 147, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1572, in _check_sample_weight_equivalence\n",
      "    assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 285, in assert_allclose_dense_sparse\n",
      "    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 239, in assert_allclose\n",
      "    np_assert_allclose(\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 1691, in assert_allclose\n",
      "    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 889, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of HuberRegressor.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00051052\n",
      "Max relative difference among violations: 7.5912896\n",
      " ACTUAL: array([-2.323244e-05,  9.999910e-01,  2.000039e+00,  1.202210e+00,\n",
      "        2.430282e+00,  9.999892e-01,  1.999998e+00,  2.000024e+00,\n",
      "        1.656899e+00,  1.999991e+00,  1.576810e+00,  1.295011e+00,\n",
      "        1.829514e+00,  1.000022e+00,  1.000070e+00])\n",
      " DESIRED: array([-2.704185e-06,  9.999971e-01,  2.000005e+00,  1.202141e+00,\n",
      "        2.430112e+00,  9.999871e-01,  1.999991e+00,  2.000007e+00,\n",
      "        1.656642e+00,  1.999968e+00,  1.576735e+00,  1.294886e+00,\n",
      "        1.829381e+00,  1.000010e+00,  9.995597e-01])\n",
      "\n",
      "❌ IsotonicRegression(): Isotonic regression input X should be a 1d array or 2d array with 1 feature\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 29, in <module>\n",
      "    check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1576, in check_sample_weight_equivalence_on_dense_data\n",
      "    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 147, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1560, in _check_sample_weight_equivalence\n",
      "    estimator_repeated.fit(X_repeated, y=y_repeated, sample_weight=None)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/isotonic.py\", line 401, in fit\n",
      "    X, y = self._build_y(X, y, sample_weight)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/isotonic.py\", line 316, in _build_y\n",
      "    self._check_input_data_shape(X)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/isotonic.py\", line 300, in _check_input_data_shape\n",
      "    raise ValueError(msg)\n",
      "ValueError: Isotonic regression input X should be a 1d array or 2d array with 1 feature\n",
      "\n",
      "❌ NuSVR(): \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of NuSVR.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00305074\n",
      "Max relative difference among violations: 0.34499784\n",
      " ACTUAL: array([1.697759e-04, 9.998989e-01, 1.999758e+00, 1.316667e+00,\n",
      "       1.577872e+00, 1.000017e+00, 2.000128e+00, 1.999764e+00,\n",
      "       1.233518e+00, 2.000170e+00, 1.400195e+00, 1.397883e+00,\n",
      "       1.430682e+00, 1.000631e+00, 9.999184e-01])\n",
      " DESIRED: array([2.591989e-04, 9.998203e-01, 1.999778e+00, 1.316390e+00,\n",
      "       1.580922e+00, 1.000526e+00, 2.000151e+00, 1.999443e+00,\n",
      "       1.233376e+00, 1.999820e+00, 1.401014e+00, 1.398252e+00,\n",
      "       1.432028e+00, 1.000286e+00, 1.000143e+00])\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 29, in <module>\n",
      "    check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1576, in check_sample_weight_equivalence_on_dense_data\n",
      "    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 147, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1572, in _check_sample_weight_equivalence\n",
      "    assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 285, in assert_allclose_dense_sparse\n",
      "    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 239, in assert_allclose\n",
      "    np_assert_allclose(\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 1691, in assert_allclose\n",
      "    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 889, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of NuSVR.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00305074\n",
      "Max relative difference among violations: 0.34499784\n",
      " ACTUAL: array([1.697759e-04, 9.998989e-01, 1.999758e+00, 1.316667e+00,\n",
      "       1.577872e+00, 1.000017e+00, 2.000128e+00, 1.999764e+00,\n",
      "       1.233518e+00, 2.000170e+00, 1.400195e+00, 1.397883e+00,\n",
      "       1.430682e+00, 1.000631e+00, 9.999184e-01])\n",
      " DESIRED: array([2.591989e-04, 9.998203e-01, 1.999778e+00, 1.316390e+00,\n",
      "       1.580922e+00, 1.000526e+00, 2.000151e+00, 1.999443e+00,\n",
      "       1.233376e+00, 1.999820e+00, 1.401014e+00, 1.398252e+00,\n",
      "       1.432028e+00, 1.000286e+00, 1.000143e+00])\n",
      "\n",
      "❌ SVR(): \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of SVR.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00266417\n",
      "Max relative difference among violations: 0.00269241\n",
      " ACTUAL: array([0.100011, 1.100002, 1.89998 , 1.327766, 1.553381, 1.099804,\n",
      "       1.900507, 1.900239, 1.253786, 1.899847, 1.414408, 1.4019  ,\n",
      "       1.440486, 1.100011, 1.099589])\n",
      " DESIRED: array([0.100281, 1.100199, 1.900164, 1.327585, 1.556046, 1.099828,\n",
      "       1.900122, 1.900199, 1.253611, 1.900075, 1.415291, 1.402263,\n",
      "       1.441794, 1.099432, 1.0997  ])\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 29, in <module>\n",
      "    check_sample_weight_equivalence_on_dense_data(est_name, est)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1576, in check_sample_weight_equivalence_on_dense_data\n",
      "    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 147, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/estimator_checks.py\", line 1572, in _check_sample_weight_equivalence\n",
      "    assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 285, in assert_allclose_dense_sparse\n",
      "    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_testing.py\", line 239, in assert_allclose\n",
      "    np_assert_allclose(\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 1691, in assert_allclose\n",
      "    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py\", line 889, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=1e-09\n",
      "Comparing the output of SVR.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n",
      "Mismatched elements: 15 / 15 (100%)\n",
      "Max absolute difference among violations: 0.00266417\n",
      "Max relative difference among violations: 0.00269241\n",
      " ACTUAL: array([0.100011, 1.100002, 1.89998 , 1.327766, 1.553381, 1.099804,\n",
      "       1.900507, 1.900239, 1.253786, 1.899847, 1.414408, 1.4019  ,\n",
      "       1.440486, 1.100011, 1.099589])\n",
      " DESIRED: array([0.100281, 1.100199, 1.900164, 1.327585, 1.556046, 1.099828,\n",
      "       1.900122, 1.900199, 1.253611, 1.900075, 1.415291, 1.402263,\n",
      "       1.441794, 1.099432, 1.0997  ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for est, e in deterministic_test_results:\n",
    "    if e is None:\n",
    "        continue\n",
    "\n",
    "    print(f\"❌ {est}: {e}\")\n",
    "    traceback.print_exception(e, file=sys.stdout)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details on other errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ KBinsDiscretizer(encode='ordinal', subsample=50): sample_weight.shape == (100,), expected (50,)!\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 44, in <module>\n",
      "    result = check_weighted_repeated_estimator_fit_equivalence(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 83, in check_weighted_repeated_estimator_fit_equivalence\n",
      "    predictions_weighted, predictions_repeated, _ = multifit_over_weighted_and_repeated(\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 260, in multifit_over_weighted_and_repeated\n",
      "    est_ref = check_pipeline_and_fit(est_ref, X_train, y_train, sample_weight_train)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 190, in check_pipeline_and_fit\n",
      "    est = est.fit(\n",
      "          ^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/pipeline.py\", line 654, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/pipeline.py\", line 588, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 921, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/preprocessing/_discretization.py\", line 254, in fit\n",
      "    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/validation.py\", line 2192, in _check_sample_weight\n",
      "    raise ValueError(\n",
      "ValueError: sample_weight.shape == (100,), expected (50,)!\n",
      "\n",
      "❌ RANSACRegressor(): Weights sum to zero, can't be normalized\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_y/lfnx34p13w3_sr2k12bjb05w0000gn/T/ipykernel_61613/841159171.py\", line 44, in <module>\n",
      "    result = check_weighted_repeated_estimator_fit_equivalence(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 83, in check_weighted_repeated_estimator_fit_equivalence\n",
      "    predictions_weighted, predictions_repeated, _ = multifit_over_weighted_and_repeated(\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 299, in multifit_over_weighted_and_repeated\n",
      "    est_weighted = check_pipeline_and_fit(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/sample-weight-audit-nondet/src/sample_weight_audit/estimator_check.py\", line 197, in check_pipeline_and_fit\n",
      "    est = est.fit(X, y, sample_weight=sample_weight)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/linear_model/_ransac.py\", line 498, in fit\n",
      "    estimator.fit(X_subset, y_subset, **fit_params_subset)\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/linear_model/_base.py\", line 635, in fit\n",
      "    X, y, X_offset, y_offset, X_scale = _preprocess_data(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/linear_model/_base.py\", line 185, in _preprocess_data\n",
      "    X_offset = _average(X, axis=0, weights=sample_weight, xp=xp)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/code/scikit-learn/sklearn/utils/_array_api.py\", line 730, in _average\n",
      "    return xp.asarray(numpy.average(a, axis=axis, weights=weights))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ogrisel/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py\", line 575, in average\n",
      "    raise ZeroDivisionError(\n",
      "ZeroDivisionError: Weights sum to zero, can't be normalized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for est, e in errors:\n",
    "    print(f\"❌ {est}: {e}\")\n",
    "    traceback.print_exception(e, file=sys.stdout)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of estimators with missing sample_weight support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARDRegression\n",
      "AdditiveChi2Sampler\n",
      "AffinityPropagation\n",
      "AgglomerativeClustering\n",
      "BernoulliRBM\n",
      "Binarizer\n",
      "Birch\n",
      "CCA\n",
      "ClassifierChain\n",
      "ColumnTransformer\n",
      "DictVectorizer\n",
      "DictionaryLearning\n",
      "FactorAnalysis\n",
      "FastICA\n",
      "FeatureAgglomeration\n",
      "FeatureHasher\n",
      "FeatureUnion\n",
      "FixedThresholdClassifier\n",
      "FunctionTransformer\n",
      "GaussianProcessClassifier\n",
      "GaussianProcessRegressor\n",
      "GaussianRandomProjection\n",
      "GenericUnivariateSelect\n",
      "HDBSCAN\n",
      "HashingVectorizer\n",
      "IncrementalPCA\n",
      "Isomap\n",
      "KNNImputer\n",
      "KNeighborsClassifier\n",
      "KNeighborsRegressor\n",
      "KNeighborsTransformer\n",
      "KernelCenterer\n",
      "KernelPCA\n",
      "LabelBinarizer\n",
      "LabelEncoder\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "Lars\n",
      "LarsCV\n",
      "LassoLars\n",
      "LassoLarsCV\n",
      "LassoLarsIC\n",
      "LatentDirichletAllocation\n",
      "LinearDiscriminantAnalysis\n",
      "LocallyLinearEmbedding\n",
      "MaxAbsScaler\n",
      "MeanShift\n",
      "MinMaxScaler\n",
      "MiniBatchDictionaryLearning\n",
      "MiniBatchNMF\n",
      "MiniBatchSparsePCA\n",
      "MissingIndicator\n",
      "MultiLabelBinarizer\n",
      "MultiTaskElasticNet\n",
      "MultiTaskElasticNetCV\n",
      "MultiTaskLasso\n",
      "MultiTaskLassoCV\n",
      "NMF\n",
      "NearestCentroid\n",
      "NeighborhoodComponentsAnalysis\n",
      "Normalizer\n",
      "Nystroem\n",
      "OPTICS\n",
      "OneHotEncoder\n",
      "OneVsOneClassifier\n",
      "OneVsRestClassifier\n",
      "OrdinalEncoder\n",
      "OrthogonalMatchingPursuit\n",
      "OrthogonalMatchingPursuitCV\n",
      "OutputCodeClassifier\n",
      "PCA\n",
      "PLSCanonical\n",
      "PLSRegression\n",
      "PLSSVD\n",
      "PassiveAggressiveClassifier\n",
      "PassiveAggressiveRegressor\n",
      "PatchExtractor\n",
      "PolynomialCountSketch\n",
      "PolynomialFeatures\n",
      "PowerTransformer\n",
      "QuadraticDiscriminantAnalysis\n",
      "QuantileTransformer\n",
      "RBFSampler\n",
      "RFE\n",
      "RFECV\n",
      "RadiusNeighborsClassifier\n",
      "RadiusNeighborsRegressor\n",
      "RadiusNeighborsTransformer\n",
      "RegressorChain\n",
      "RobustScaler\n",
      "SelectFdr\n",
      "SelectFpr\n",
      "SelectFromModel\n",
      "SelectFwe\n",
      "SelectKBest\n",
      "SelectPercentile\n",
      "SelfTrainingClassifier\n",
      "SequentialFeatureSelector\n",
      "SimpleImputer\n",
      "SkewedChi2Sampler\n",
      "SparseCoder\n",
      "SparsePCA\n",
      "SparseRandomProjection\n",
      "SpectralClustering\n",
      "TSNE\n",
      "TargetEncoder\n",
      "TfidfTransformer\n",
      "TheilSenRegressor\n",
      "TransformedTargetRegressor\n",
      "TruncatedSVD\n",
      "TunedThresholdClassifierCV\n",
      "VarianceThreshold\n"
     ]
    }
   ],
   "source": [
    "for est_name in missing_sample_weight_support:\n",
    "    print(est_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
